{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se presentan las librerias utilizdas, pandas es una libreria de analisis de datos ; Tweet_mx api  desarollada para administrar las querrys y la extracion de tweets tambien sera la clase utilizada para la administracion de la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from  Tweet_mx import Tweet_mx\n",
    "from Base_d import Bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while connecting to MySQL 1045 (28000): Access denied for user 'lucsa'@'localhost' (using password: NO)\n",
      "Connected to MySQL Server version  8.0.32-0ubuntu0.22.04.2\n",
      "You're connected to database:  ('tweet_mx',)\n"
     ]
    }
   ],
   "source": [
    "bd=Bd()\n",
    "bd.set_user(\"lVC$4\")\n",
    "bd.set_host(\"localhost\")\n",
    "bd.set_datebase(\"tweet_mx\")\n",
    "bd.set_passwoord(\"anesartnoc\")\n",
    "bd.get_connection()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se prensentan los campos de los datos dentro de un tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url                              object\n",
       "date                datetime64[ns, UTC]\n",
       "rawContent                       object\n",
       "renderedContent                  object\n",
       "id                                int64\n",
       "user                             object\n",
       "replyCount                        int64\n",
       "retweetCount                      int64\n",
       "likeCount                         int64\n",
       "quoteCount                        int64\n",
       "conversationId                    int64\n",
       "lang                             object\n",
       "source                           object\n",
       "sourceUrl                        object\n",
       "sourceLabel                      object\n",
       "links                            object\n",
       "media                            object\n",
       "retweetedTweet                   object\n",
       "quotedTweet                      object\n",
       "inReplyToTweetId                 object\n",
       "inReplyToUser                    object\n",
       "mentionedUsers                   object\n",
       "coordinates                      object\n",
       "place                            object\n",
       "hashtags                         object\n",
       "cashtags                         object\n",
       "card                             object\n",
       "viewCount                         int64\n",
       "vibe                             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = Tweet_mx()\n",
    "\n",
    "q.set_text(\"Guillermo del Toro \")\n",
    "q.set_since(\"2023-03-13\")\n",
    "\n",
    "limit  = 1\n",
    "entero = 1\n",
    "tweets=[]\n",
    "\n",
    "tweets=q.extraer_tweets(limit,entero)\n",
    "tweets_df1 = pd.DataFrame(tweets)\n",
    "tweets_df1.dtypes\n",
    "tweets[0].coordinates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "busqueda de los tranding topic en la fecha, se descartan los datos inesesarios y se crea un data frame con los siguientes campos  IdTweet, Datetime, Tweet, Username; estos camopos seran posteriormente subidos a la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m entero \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      7\u001b[0m tweets\u001b[39m=\u001b[39m[]\n\u001b[0;32m----> 9\u001b[0m tweets\u001b[39m=\u001b[39mq\u001b[39m.\u001b[39;49mextraer_tweets(limit,entero)\n\u001b[1;32m     10\u001b[0m tweets_df1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(tweets , columns\u001b[39m=\u001b[39m[ \u001b[39m'\u001b[39m\u001b[39mTweet Id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDatetime\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTweet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mUsername\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m tweets_df1\u001b[39m.\u001b[39mhead\n",
      "File \u001b[0;32m~/Documentos/Nao.py(data_analyst)/captura/Tweet_mx.py:80\u001b[0m, in \u001b[0;36mTweet_mx.extraer_tweets\u001b[0;34m(self, cont, entero)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextraer_tweets\u001b[39m( \u001b[39mself\u001b[39m, cont, entero ):\n\u001b[1;32m     79\u001b[0m     tweets\u001b[39m=\u001b[39m[]\n\u001b[0;32m---> 80\u001b[0m     \u001b[39mfor\u001b[39;00m tweet \u001b[39min\u001b[39;00m sn_twitter\u001b[39m.\u001b[39mTwitterSearchScraper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_querry())\u001b[39m.\u001b[39mget_items():\n\u001b[1;32m     81\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tweets)\u001b[39m==\u001b[39m cont:\n\u001b[1;32m     82\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/snscrape/modules/twitter.py:1657\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1654\u001b[0m params \u001b[39m=\u001b[39m paginationParams\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m   1655\u001b[0m \u001b[39mdel\u001b[39;00m params[\u001b[39m'\u001b[39m\u001b[39mcursor\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m-> 1657\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_api_data(\u001b[39m'\u001b[39m\u001b[39mhttps://api.twitter.com/2/search/adaptive.json\u001b[39m\u001b[39m'\u001b[39m, _TwitterAPIType\u001b[39m.\u001b[39mV2, params, paginationParams, cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cursor):\n\u001b[1;32m   1658\u001b[0m \t\u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_v2_timeline_instructions_to_tweets_or_users(obj)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/snscrape/modules/twitter.py:761\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[0;34m(self, endpoint, apiType, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    760\u001b[0m \t_logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRetrieving scroll page \u001b[39m\u001b[39m{\u001b[39;00mcursor\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 761\u001b[0m \tobj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_api_data(endpoint, apiType, reqParams)\n\u001b[1;32m    762\u001b[0m \t\u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    764\u001b[0m \t\u001b[39m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/snscrape/modules/twitter.py:727\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[0;34m(self, endpoint, apiType, params)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mif\u001b[39;00m apiType \u001b[39mis\u001b[39;00m _TwitterAPIType\u001b[39m.\u001b[39mGRAPHQL:\n\u001b[1;32m    726\u001b[0m \tparams \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mparse\u001b[39m.\u001b[39murlencode({k: json\u001b[39m.\u001b[39mdumps(v, separators \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39mitems()}, quote_via \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mparse\u001b[39m.\u001b[39mquote)\n\u001b[0;32m--> 727\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get(endpoint, params \u001b[39m=\u001b[39;49m params, headers \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apiHeaders, responseOkCallback \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_api_response)\n\u001b[1;32m    728\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    729\u001b[0m \tobj \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/snscrape/base.py:251\u001b[0m, in \u001b[0;36mScraper._get\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 251\u001b[0m \t\u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\u001b[39m'\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/snscrape/base.py:242\u001b[0m, in \u001b[0;36mScraper._request\u001b[0;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[1;32m    240\u001b[0m \t\tsleepTime \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mattempt \u001b[39m# exponential backoff: sleep 1 second after first attempt, 2 after second, 4 after third, etc.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \t\t_logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWaiting \u001b[39m\u001b[39m{\u001b[39;00msleepTime\u001b[39m:\u001b[39;00m\u001b[39m.0f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 242\u001b[0m \t\ttime\u001b[39m.\u001b[39;49msleep(sleepTime)\n\u001b[1;32m    243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m \tmsg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retries\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m requests to \u001b[39m\u001b[39m{\u001b[39;00mreq\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m failed, giving up.\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q = Tweet_mx()\n",
    "\n",
    "q.set_text(\"Ignacio LÃ³pez Tarso\")\n",
    "q.set_since(\"2023-03-11\")\n",
    "limit  = 100\n",
    "entero = 0\n",
    "tweets=[]\n",
    "\n",
    "tweets=q.extraer_tweets(limit,entero)\n",
    "tweets_df1 = pd.DataFrame(tweets , columns=[ 'Tweet Id', 'Datetime', 'Tweet', 'Username'])\n",
    "tweets_df1.head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
